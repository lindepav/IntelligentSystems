---
title: "Assigment 2"
author: "Pavel Linder, Nikita Brancatisano"
date: "12/26/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r libraries, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
#install.packages("ngram")
#install.packages("ggplot2")
library(gtools) # if problems calling library, install.packages("gtools", dependencies = T)
library(stringr)
library(tm) # framework for text mining; it loads NLP package
library(cluster)
library(proxy)
library(ngram)
```

## 0. Read input
```{r import, echo=TRUE}
train = read.table(file = 'train.tsv', sep = '\t', header = TRUE, stringsAsFactors = FALSE)
#train <- train[-c(1),]
test = read.table(file = 'test.tsv', sep = '\t', header = TRUE)
length(which(!complete.cases(train)))
head(train$text_a)
```


# 1. Cleaning data
## Remove punctuation and stopwords
```{r punctuation, echo=TRUE}
train$text_a = as.character(train$text_a)
train$text_a = tm::removePunctuation(train$text_a)
train$text_a = tm::removeWords(x = train$text_a, stopwords(kind = "SMART"))
train$text_a = tm::stripWhitespace(train$text_a)

#train$text_a = tolower(train$text_a)
word_count <- lapply(train$text_a, wordcount)
length(which(word_count > 50))
length(which(word_count < 50))
length(which(word_count == 0))
length(train$text_a)
train <- train[which(word_count < 50),]
word_count <- lapply(train$text_a, wordcount)
length(train$text_a)
train <- train[which(word_count > 0),]
length(train$text_a)
head(train$text_a)
```

## Anonymize proper nouns

```{r import2, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
#install.packages("rJava")
#install.packages("openNLP")
#install.packages("openNLPmodels.en", dependencies=TRUE, repos = "http://datacube.wu.ac.at/")
library(NLP)
library(rJava)
library(openNLP)
library(openNLPmodels.en)
library(digest)
#detach("package:ggplot2", unload=TRUE)
```

```{r anonymize, echo=TRUE}
n <- length(train$text_a)
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
pos_ann = Maxent_POS_Tag_Annotator()

for (i in 1:n) {
  while(1) {
    doc <- as.String(train$text_a[[i]])
    wordAnnotation <- annotate(doc, list(sent_ann, word_ann))
    POSAnnotation <- annotate(doc, pos_ann, wordAnnotation)
    POSWords <- subset(POSAnnotation, type == "word")  
    POSTags <- vector()
    for (j in 1:length(POSWords$features))
      POSTags <- c(POSTags, POSWords$features[[j]]$POS)
    tokenPOS <- cbind(doc[POSWords], POSTags)
    ppn_idx <- which(tokenPOS[,2] == "NNP", 1)
    if (length(ppn_idx) == 0) {
      break;
    }
    words <- subset(wordAnnotation, type == "word")  
    hashed <- digest(tokenPOS[ppn_idx, 1], "xxhash32")
    ppn <- words[ppn_idx]
    train$text_a[[i]] <- gsub(doc[ppn$start,ppn$end], hashed, doc)
  }
}

head(train$text_a)
```

## Remove unknown symbols (non UTF-8 characters)
```{r symbols, echo=TRUE}
train$text_a <- iconv(train$text_a, to='UTF-8', sub='byte')
length(train$text_a)
head(train$text_a)
```

# 2. Exploration
### I. Plot the frequency of words (without stemmization)
```{r frequency1, echo=TRUE}
library(ggplot2)
corpus <- Corpus(VectorSource(train$text_a)) # turn into corpus
tdm <- TermDocumentMatrix(corpus)

wordFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
qplot(seq(length(wordFreq)),sort(wordFreq), xlab = "index", ylab = "Frequency")

findFreqTerms(tdm, lowfreq=50)
mostFreq <- subset(wordFreq, wordFreq >= 50)
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")

length(wordFreq)

length(wordFreq[wordFreq<10])

length(wordFreq[wordFreq<5])

length(wordFreq[wordFreq==1])


freq <- sort(unique(wordFreq), decreasing=FALSE)
occ <- vector()
for (i in 1:length(freq)) {
  occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies")

```

### I. Plot the frequency of words (with stemmization)
```{r stemmization1, echo=TRUE}
stemmed <- stemDocument(train$text_a, language = "english")
corpus2 <- Corpus(VectorSource(stemmed)) # turn into corpus
```
```{r freq2, echo=FALSE}
tdm2 <- TermDocumentMatrix(corpus2)
wordFreq <- sort(rowSums(as.matrix(tdm2)), decreasing=TRUE)
mostFreq <- subset(wordFreq, wordFreq >= 50)
```
```{r stemmization2, echo=TRUE}
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")

length(wordFreq)

length(wordFreq[wordFreq<10])

length(wordFreq[wordFreq<5])

length(wordFreq[wordFreq==1])
```
```{r freq3, echo=FALSE}
freq <- sort(unique(wordFreq), decreasing=FALSE)
occ <- vector()
for (i in 1:length(freq)) {
  occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
```
```{r stemmization3, echo=TRUE}
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies")
```

## II. Perform a clustering on the vectorized document space
We will use Weighted TF-IDF as a way to represent the document space:
```{r cluster1, echo=TRUE}
tdm <- tm::DocumentTermMatrix(corpus2) 
tdm.tfidf <- tm::weightTfIdf(tdm)

tdm.tfidf <- tm::removeSparseTerms(tdm.tfidf, 0.999)  # sparsity being not well handled overall in R
tfidf.matrix <- as.matrix(tdm.tfidf) 
```
Afterwards, we perform kmeans algorithm to cluster in {2,4,8,16} classes.
```{r cluster2, echo=TRUE}
cluster2 <- kmeans(tfidf.matrix, centers=2)
cluster4 <- kmeans(tfidf.matrix, centers=4)
cluster8 <- kmeans(tfidf.matrix, centers=8)
cluster16 <- kmeans(tfidf.matrix, centers=16)

cluster2.master <- cluster2$cluster
cluster4.master <- cluster4$cluster
cluster8.master <- cluster8$cluster
cluster16.master <- cluster16$cluster
```
We perform Classical multidimensional scaling (SMC) to map the data (distance matrix) into 2D dimension and then visualize it.
```{r cluster_visualize, echo=TRUE}
dist.matrix = proxy::dist(tfidf.matrix, method = "cosine") 
points <- cmdscale(dist.matrix, k = 2) 
previous.par <- par(mfrow=c(2,2), mar = rep(1.5, 4)) 
color <- grDevices::colors()[grep('gr(a|e)y', grDevices::colors(), invert = T)]

my_palette = sample(color, 2)
plot(points, main = 'K-Means clustering (k=2)', col = my_palette[as.factor(cluster2.master)], 
     mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), 
     xaxt = 'n', yaxt = 'n', xlab = '', ylab = '') 
legend("topright", sprintf("%s",seq(1,2)), fill = my_palette[1:2])
clusplot(points, cluster2.master, main='K-Means clustering (k=2)', color=TRUE, shade=TRUE, labels=2, lines=0)

my_palette = sample(color, 4)
plot(points, main = 'K-Means clustering (k=4)', col = my_palette[as.factor(cluster4.master)], 
     mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), 
     xaxt = 'n', yaxt = 'n', xlab = '', ylab = '') 
legend("topright", sprintf("%s",seq(1,4)), fill = my_palette[1:4])
clusplot(points, cluster4.master, main = 'K-Means clustering (k=4)', color=TRUE, shade=TRUE, labels=2, lines=0)

my_palette = sample(color, 8)
plot(points, main = 'K-Means clustering (k=8)', col = my_palette[as.factor(cluster8.master)], 
     mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), 
     xaxt = 'n', yaxt = 'n', xlab = '', ylab = '') 
legend("topright", sprintf("%s",seq(1,8)), fill = my_palette[1:8])
clusplot(points, cluster8.master, main = 'K-Means clustering (k=8)', color=TRUE, shade=TRUE, labels=2, lines=0)

my_palette = sample(color, 16)
plot(points, main = 'K-Means clustering (k=16)', col = my_palette[as.factor(cluster16.master)],  
     mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), 
     xaxt = 'n', yaxt = 'n', xlab = '', ylab = '') 
legend("topright", sprintf("%s",seq(1,16)), fill = my_palette[1:16])
clusplot(points, cluster16.master, main = 'K-Means clustering (k=16)', color=TRUE, shade=TRUE, labels=2, lines=0)

par(previous.par) # recovering the original plot space parameters 
```
```{r class_coloring, echo=TRUE}
points <- cmdscale(dist.matrix, k = 2) 
colors <- c('red', 'blue')

plot(points, main = 'Documents with class labels', col = colors[as.factor(train$label)], 
     mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), 
     xaxt = 'n', yaxt = 'n', xlab = '', ylab = '') 
legend("topright", c('ham', 'insult'), fill = colors[1:2])
```

```{r import3, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
#install.packages("openNLPmodels.en", dependencies=TRUE, repos = "http://datacube.wu.ac.at/")
library(NLP)
library(rJava)
library(openNLP)
library(openNLPmodels.en)
detach("package:ggplot2", unload=TRUE)
```

```{r pos, echo=TRUE}
docs <- stemmed
n <- length(docs)
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
pos_ann = Maxent_POS_Tag_Annotator()

docsPOS <- list()
for (i in 1:n) {
  doc <-  as.String(docs[[i]])
  wordAnnotation <- annotate(doc, list(sent_ann, word_ann))
  POSAnnotation <- annotate(doc, pos_ann, wordAnnotation)
  POSWords <- subset(POSAnnotation, type == "word")  
  POSTags <- vector()
  for (j in 1:length(POSWords$features))
    POSTags <- c(POSTags, POSWords$features[[j]]$POS)
  docsPOS[[i]] <- list(POSTags)
}

head(docsPOS)
head(stemmed)
length(docsPOS)
```

```{r import4, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
#install.packages("ipred")
#install.packages("MLmetrics")
library(ipred)
library(caret)
library(tm)
library(MLmetrics)
library(rpart)
library(rpart.plot)

```

```{r analyze, echo=TRUE}
table(train$label)
train$text_a = stemmed
ins <- sum(train$label)
notins <- length(train$label) - ins
torem <- notins - ins
ratio <- ins/notins
count <- 0
numb <- train[3,1]
for (i in 1:length(train$label)){
  if(count > torem){
    break
  }
  
  if(train[i,1] == numb){
    train[i,1] <- NA
    count <- count + 1
  }
}
train <- train[complete.cases(train),]
```

```{r preprocess_test, echo=TRUE}
test$text_a = as.character(test$text_a)
test$text_a = tm::removePunctuation(test$text_a)
test$text_a = tm::removeWords(x = test$text_a, stopwords(kind = "SMART"))
test$text_a = tm::stripWhitespace(test$text_a)
test$text_a <- iconv(test$text_a, to='UTF-8', sub='byte')
stemmedtest <- stemDocument(test$text_a, language = "english")
corpustest <- Corpus(VectorSource(stemmedtest)) # turn into corpus

tdmtest <- tm::DocumentTermMatrix(corpustest) 
tdmtest.tfidf <- tm::weightTfIdf(tdmtest)
tdmtest.tfidf <- tm::removeSparseTerms(tdmtest.tfidf, 0.999) 
tfidftest.matrix <- as.matrix(tdmtest.tfidf)
```

```{r model, echo = TRUE}
sample_idx = sample(seq(1, length(train$text_a)), 100)
train <- train[sample_idx,]
table(train$label)

stemmed <- stemDocument(train$text_a, language = "english")
corpus2 <- Corpus(VectorSource(stemmed)) # turn into corpus
tdm <- tm::DocumentTermMatrix(corpus2) 
tdm.tfidf <- tm::weightTfIdf(tdm)
tdm.tfidf <- tm::removeSparseTerms(tdm.tfidf, 0.999)  # sparsity being not well handled overall in R
tfidf.matrix <- as.matrix(tdm.tfidf) 

train$lbl <- train$label
avector <- as.vector(train['lbl'])
final <- cbind(tfidf.matrix, avector)
final <- as.data.frame(final)
finalna <- final
final$lbl=ifelse(final$lbl==0,"No","Yes") 
final$lbl <- as.factor(final$lbl)

dat <- twoClassSim(200)
f1 <- function(data, lev = NULL, model = NULL) {
  f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])
  c(F1 = f1_val)
}

train.control <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 3,
                          classProbs = TRUE,
                          #sampling = "smote",
                          summaryFunction = f1,
                          search = "grid")


fitRf <- train(lbl ~ ., data = final, method = 'rf',tuneLength = 5, metric = "F1",
             trControl = trainControl(summaryFunction = f1, 
                                    classProbs = TRUE)) 

fitSvm <- train(lbl ~ ., data = final, method = 'svmLinear',scale=F, tuneLength = 5, metric = "F1",
             trControl = train.control) 


test.matrix <- as.data.frame(tfidftest.matrix)
cols <- colnames(final)
Missing <- setdiff(cols, names(test.matrix))
test.matrix[Missing] <- 0
test.matrix <- test.matrix[cols]

predRf = predict(fitRf, newdata=test.matrix)
fitRf
predSvm = predict(fitSvm, newdata=test.matrix)
fitSvm

test$label=ifelse(test$label==0,"No","Yes") 
test$label <- as.factor(test$label)

con.matrix.rf<-confusionMatrix(predRf, test$label)
print(con.matrix.rf)

con.matrix.svm<-confusionMatrix(predSvm, test$label)
print(con.matrix.svm)
```

# 4. Understanding
## Perform feature ranking 
We will use only terms which occur in more than one document and which correlation with other terms is high:  
```{r filtering, echo=TRUE}
# load the library
library(mlbench)
library(caret)

# get rid of words which are only in 1 document
tfidf.matrix <- tfidf.matrix[,-which(rowSums(as.matrix(tdm2)) == 1)]
dim(tfidf.matrix)

# calculate correlation matrix
correlationMatrix <- cor(tfidf.matrix)
# summarize the correlation matrix
dim(correlationMatrix)
# find attributes that are highly corrected 
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# how many attributes stays (we get indices as 'highlyCorrelated')
length(highlyCorrelated)
dim(tfidf.matrix)
tfidf.matrix <- tfidf.matrix[,highlyCorrelated]
dim(tfidf.matrix)

```

We will use the TF-IDF matrix to calculate information gain:
```{r feature-selection, echo=TRUE}
#install.packages('ggpubr')
library(mlr)
library(ggpubr)
train$lbl <- train$label
avector <- as.vector(train['lbl'])
final <- cbind(tfidf.matrix, avector)
final <- as.data.frame(final)
final$lbl=ifelse(final$lbl==0,"No","Yes") 
final$lbl <- as.factor(final$lbl)
colnames(final) <- make.names(colnames(final),unique = T)
label.task <- makeClassifTask(data=final, target='lbl')

melt <- reshape2::melt
fv = generateFilterValuesData(label.task, method ="anova.test")

fv$data <- fv$data[order(-fv$data$value),]
fv.plot <- fv
fv.plot$data <- head(fv.plot$data, 20)
fv.plot$data
plotFilterValues(fv.plot)
```

## Extract feature importances from a wrapper method
```{r varimp, echo=TRUE}
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- caret::train(lbl~., data=final, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)$importance
# summarize importance
print(importance)
# plot importance
plot(varImp(model, scale=FALSE), top = 20)
```

## Compare the two feature rankings 

```{r jaccard, echo=TRUE}
library(ggplot2)
get_jaccard <- function(A, B) {
  return(length(intersect(A,B)) / length(union(A,B)))  
}

A <- fv$data$name
B <- rownames(importance[order(-importance$Yes),])
length(A) == length(B)

Jaccard.score <- c()
for (i in 1:length(A)) {
  Jaccard.score[i] <- get_jaccard(A[1:i], B[1:i])
}
n <- seq(1, length(A), 1)
qplot(n, Jaccard.score)

```