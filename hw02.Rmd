---
title: "Assigment 2"
author: "Pavel Linder, Nikita Brancatisano"
date: "12/26/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r libraries, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
install.packages("proxy")
library(gtools) # if problems calling library, install.packages("gtools", dependencies = T)
library(stringr)
library(tm) # framework for text mining; it loads NLP package
library(ggplot2)
library(cluster)
library(proxy)
```

## 0. Read input
```{r import, echo=TRUE}
train = read.table(file = 'train.tsv', sep = '\t', header = TRUE, stringsAsFactors = FALSE)
train <- train[-c(1),]
test = read.table(file = 'test.tsv', sep = '\t', header = TRUE)

length(which(!complete.cases(train)))
head(train$text_a)
```

# 1. Cleaning data
## Remove punctuation and stopwords
```{r punctuation, echo=TRUE}
train$text_a = as.character(train$text_a)
train$text_a = tm::removePunctuation(train$text_a)
train$text_a = tm::removeWords(x = train$text_a, stopwords(kind = "SMART"))
train$text_a = tm::stripWhitespace(train$text_a)
#train$text_a = tolower(train$text_a)
head(train$text_a)
```

## Anonymize proper nouns

```{r import2, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
install.packages("openNLPmodels.en", dependencies=TRUE, repos = "http://datacube.wu.ac.at/")



library(NLP)
install.packages("rJava")
library(rJava)
install.packages("openNLP")
library(openNLP)
install.packages("openNLPmodels.en")
install.packages("ggplot2")
library(openNLPmodels.en)
library(digest)
#detach("package:ggplot2", unload=TRUE)
library(ggplot2)
```

```{r anonymize, echo=TRUE}
n <- length(train$text_a)
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
pos_ann = Maxent_POS_Tag_Annotator()

for (i in 1:n) {
  doc <- as.String(train$text_a[[1]])
  wordAnnotation <- annotate(doc, list(sent_ann, word_ann))
  POSAnnotation <- annotate(doc, pos_ann, wordAnnotation)
  POSWords <- subset(POSAnnotation, type == "word")  
  POSTags <- vector()
  for (j in 1:length(POSWords$features))
    POSTags <- c(POSTags, POSWords$features[[j]]$POS)
  
  tokenPOS <- cbind(doc[POSWords], POSTags)
  ppn_idx <- which(tokenPOS[,2] == "NNP", 1)
    
  tokenPOS[ppn_idx] <- digest(tokenPOS[ppn_idx,1], "md5")
  #new <- 
  #train$text_a[[i]] <- list(POSTags)
}
```

## Remove unknown symbols (non UTF-8 characters)
```{r symbols, echo=TRUE}
train$text_a <- iconv(train$text_a, to='UTF-8', sub='byte')
length(train$text_a)
train <- train[train$text_a != " ",]
length(train$text_a)
head(train$text_a)
```

# 2. Exploration
### I. Plot the frequency of words (without stemmization)
```{r frequency1, echo=TRUE}
corpus <- Corpus(VectorSource(train$text_a)) # turn into corpus
tdm <- TermDocumentMatrix(corpus)

wordFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
qplot(seq(length(wordFreq)),sort(wordFreq), xlab = "index", ylab = "Frequency")

findFreqTerms(tdm, lowfreq=50)
mostFreq <- subset(wordFreq, wordFreq >= 50)
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")

length(wordFreq)

length(wordFreq[wordFreq<10])

length(wordFreq[wordFreq<5])

length(wordFreq[wordFreq==1])


freq <- sort(unique(wordFreq), decreasing=FALSE)
occ <- vector()
for (i in 1:length(freq)) {
  occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies")

```

### I. Plot the frequency of words (with stemmization)
```{r stemmization1, echo=TRUE}
stemmed <- stemDocument(train$text_a, language = "english")
corpus2 <- Corpus(VectorSource(stemmed)) # turn into corpus
```
```{r freq2, echo=FALSE}
tdm2 <- TermDocumentMatrix(corpus2)
wordFreq <- sort(rowSums(as.matrix(tdm2)), decreasing=TRUE)
mostFreq <- subset(wordFreq, wordFreq >= 50)
```
```{r stemmization2, echo=TRUE}
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")

length(wordFreq)

length(wordFreq[wordFreq<10])

length(wordFreq[wordFreq<5])

length(wordFreq[wordFreq==1])
```
```{r freq3, echo=FALSE}
freq <- sort(unique(wordFreq), decreasing=FALSE)
occ <- vector()
for (i in 1:length(freq)) {
  occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
```
```{r stemmization3, echo=TRUE}
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies")
```

## II. Perform a clustering on the vectorized document space
We will use Weighted TF-IDF as a way to represent the document space:
```{r cluster1, echo=TRUE}
tdm <- tm::DocumentTermMatrix(corpus2) 
tdm.tfidf <- tm::weightTfIdf(tdm)
tdm.tfidf <- tm::removeSparseTerms(tdm.tfidf, 0.999)  # sparsity being not well handled overall in R
tfidf.matrix <- as.matrix(tdm.tfidf) 
```
Afterwards, we perform kmeans algorithm to cluster in {2,4,8,16} classes.
```{r cluster2, echo=TRUE}
cluster2 <- kmeans(tfidf.matrix, centers=2)
cluster4 <- kmeans(tfidf.matrix, centers=4)
cluster8 <- kmeans(tfidf.matrix, centers=8)
cluster16 <- kmeans(tfidf.matrix, centers=16)

cluster2.master <- cluster2$cluster
cluster4.master <- cluster4$cluster
cluster8.master <- cluster8$cluster
cluster16.master <- cluster16$cluster
```
We perform Classical multidimensional scaling (SMC) to map the data (distance matrix) into 2D dimension and then visualize it.
```{r cluster_visualize, echo=TRUE}
dist.matrix = proxy::dist(tfidf.matrix, method = "cosine") 
points <- cmdscale(dist.matrix, k = 2) 
previous.par <- par(mfrow=c(2,2), mar = rep(1.5, 4)) 
color <- grDevices::colors()[grep('gr(a|e)y', grDevices::colors(), invert = T)]

my_palette = sample(color, 2)
plot(points, main = 'K-Means clustering (k=2)', col = my_palette[as.factor(cluster2.master)], 
     mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), 
     xaxt = 'n', yaxt = 'n', xlab = '', ylab = '') 
legend("topright", sprintf("%s",seq(1,2)), fill = my_palette[1:2])
clusplot(points, cluster2.master, main='K-Means clustering (k=2)', color=TRUE, shade=TRUE, labels=2, lines=0)

my_palette = sample(color, 4)
plot(points, main = 'K-Means clustering (k=4)', col = my_palette[as.factor(cluster4.master)], 
     mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), 
     xaxt = 'n', yaxt = 'n', xlab = '', ylab = '') 
legend("topright", sprintf("%s",seq(1,4)), fill = my_palette[1:4])
clusplot(points, cluster4.master, main = 'K-Means clustering (k=4)', color=TRUE, shade=TRUE, labels=2, lines=0)

my_palette = sample(color, 8)
plot(points, main = 'K-Means clustering (k=8)', col = my_palette[as.factor(cluster8.master)], 
     mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), 
     xaxt = 'n', yaxt = 'n', xlab = '', ylab = '') 
legend("topright", sprintf("%s",seq(1,8)), fill = my_palette[1:8])
clusplot(points, cluster8.master, main = 'K-Means clustering (k=8)', color=TRUE, shade=TRUE, labels=2, lines=0)

my_palette = sample(color, 16)
plot(points, main = 'K-Means clustering (k=16)', col = my_palette[as.factor(cluster16.master)],  
     mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), 
     xaxt = 'n', yaxt = 'n', xlab = '', ylab = '') 
legend("topright", sprintf("%s",seq(1,16)), fill = my_palette[1:16])
clusplot(points, cluster16.master, main = 'K-Means clustering (k=16)', color=TRUE, shade=TRUE, labels=2, lines=0)

par(previous.par) # recovering the original plot space parameters 
```
```{r class_coloring, echo=TRUE}
points <- cmdscale(dist.matrix, k = 2) 
colors <- c('red', 'blue')

plot(points, main = 'Documents with class labels', col = colors[as.factor(train$label)], 
     mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), 
     xaxt = 'n', yaxt = 'n', xlab = '', ylab = '') 
legend("topright", c('ham', 'insult'), fill = colors[1:2])
```

```{r import3, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
#install.packages("openNLPmodels.en", dependencies=TRUE, repos = "http://datacube.wu.ac.at/")
library(NLP)
library(rJava)
library(openNLP)
library(openNLPmodels.en)
detach("package:ggplot2", unload=TRUE)
```

```{r pos, echo=TRUE}
docs <- stemmed
n <- length(docs)
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
pos_ann = Maxent_POS_Tag_Annotator()

docsPOS <- list()
for (i in 1:n) {
  doc <-  as.String(docs[[i]])
  wordAnnotation <- annotate(doc, list(sent_ann, word_ann))
  POSAnnotation <- annotate(doc, pos_ann, wordAnnotation)
  POSWords <- subset(POSAnnotation, type == "word")  
  POSTags <- vector()
  for (j in 1:length(POSWords$features))
    POSTags <- c(POSTags, POSWords$features[[j]]$POS)
  docsPOS[[i]] <- list(POSTags)
}

head(docsPOS)
head(stemmed)
length(docsPOS)
```

```{r model, echo = TRUE}
ins <- sum(train$label)
notins <- length(train$label)
ratio <- ins/notins

install.packages("ipred")
library(tidyverse)
library(ipred)

library(tidytext)
library(stringr)
library(caret)
library(tm)

control <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)

avector <- as.vector(train['label'])
final <- cbind(tfidf.matrix, avector)
final <- as.data.frame(final)
final$label <- as.factor(final$label)

spamLog = glm(label~., data=final, family="binomial")


test$text_a = as.character(test$text_a)
test$text_a = tm::removePunctuation(test$text_a)
test$text_a = tm::removeWords(x = test$text_a, stopwords(kind = "SMART"))
test$text_a = tm::stripWhitespace(test$text_a)
stemmedtest <- stemDocument(test$text_a, language = "english")
corpustest <- Corpus(VectorSource(stemmedtest)) # turn into corpus

tdmtest <- tm::DocumentTermMatrix(corpustest) 
tdmtest.tfidf <- tm::weightTfIdf(tdmtest)
tdmtest.tfidf <- tm::removeSparseTerms(tdmtest.tfidf, 0.999)  # sparsity being not well handled overall in R
tfidftest.matrix <- as.matrix(tdmtest.tfidf)
install.packages("janitor")
library(janitor)
tfidftest.matrix <- clean_names(tfidftest.matrix)
pred<-predict(fit,tfidftest.matrix)
```