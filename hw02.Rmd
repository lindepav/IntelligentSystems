---
title: "Assigment 2"
author: "Pavel Linder, Nikita Brancatisano"
date: "12/26/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r libraries, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
library(gtools) # if problems calling library, install.packages("gtools", dependencies = T)
#library(qdap) # qualitative data analysis package (it masks %>%)
library(stringr)
library(tm) # framework for text mining; it loads NLP package
library(ggplot2)
```

## 0. Read input
```{r import, echo=TRUE}
train = read.table(file = 'train.tsv', sep = '\t', header = TRUE, stringsAsFactors = FALSE)
test = read.table(file = 'test.tsv', sep = '\t', header = TRUE)
length(which(!complete.cases(train)))
train$text_a[1:3]
```

# 1. Cleaning data
## Remove punctuation and stopwords (?TODO: tolower)
```{r punctuation, echo=TRUE}
train$text_a = as.character(train$text_a)
train$text_a = tm::removePunctuation(train$text_a)
train$text_a = tm::removeWords(x = train$text_a, stopwords(kind = "SMART"))
train$text_a = tm::stripWhitespace(train$text_a)
train$text_a[1:3]
```

## Anonymize proper nouns
```{r anonymize, echo=TRUE}

```

## Remove unknown symbols (non UTF-8 characters)
```{r symbols, echo=TRUE}
#train$text_a = str_replace_all(train$text_a, "[^[:alnum:],[:blank:]/\\-]", "")
train$text_a <- iconv(train$text_a, to='UTF-8', sub='byte')
train$text_a[1:3]
```

# 2. Exploration
### I. Plot the frequency of words (without stemmization)
```{r frequency1, echo=TRUE}
corpus <- Corpus(VectorSource(train$text_a)) # turn into corpus
tdm <- TermDocumentMatrix(corpus)

wordFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
qplot(seq(length(wordFreq)),sort(wordFreq), xlab = "index", ylab = "Frequency")

findFreqTerms(tdm, lowfreq=50)
mostFreq <- subset(wordFreq, wordFreq >= 50)
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")

length(wordFreq)

length(wordFreq[wordFreq<10])

length(wordFreq[wordFreq<5])

length(wordFreq[wordFreq==1])


freq <- sort(unique(wordFreq), decreasing=FALSE)
occ <- vector()
for (i in 1:length(freq)) {
  occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies")

```

### I. Plot the frequency of words (with stemmization)
```{r stemmization1, echo=TRUE}
stemmed <- stemDocument(train$text_a, language = "english")
corpus2 <- Corpus(VectorSource(stemmed)) # turn into corpus
```
```{r freq2, echo=FALSE}
tdm2 <- TermDocumentMatrix(corpus2)
wordFreq <- sort(rowSums(as.matrix(tdm2)), decreasing=TRUE)
mostFreq <- subset(wordFreq, wordFreq >= 50)
```
```{r stemmization2, echo=TRUE}
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")

length(wordFreq)

length(wordFreq[wordFreq<10])

length(wordFreq[wordFreq<5])

length(wordFreq[wordFreq==1])
```
```{r freq3, echo=FALSE}
freq <- sort(unique(wordFreq), decreasing=FALSE)
occ <- vector()
for (i in 1:length(freq)) {
  occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
```
```{r stemmization3, echo=TRUE}
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies")
```

## II. Perform a clustering on the vectorized document space
We will use Weighted TF-IDF as a way to represent the document space:
```{r cluster, echo=TRUE}
tfidf <- as.matrix(tm::DocumentTermMatrix(corpus), control = list(weighting=weightTfIdf)) 
tfidf <- tm::removeSparseTerms(tfidf, 0.7) 

k <- c(2, 4, 8, 16)
for (j in 1:4) {
  kmeansResult <- kmeans(tfidf, k[j]) 
  
  # Find the most popular words in every cluster
  cat("for k = ", k[j], ":\n")
  for (i in 1:k[j]) {
    s <- sort(kmeansResult$centers[i,], decreasing=TRUE)
    
    cat(names(s)[1:10], "\n")
  }
  cat("\n")
}

```
