test = read.table(file = 'test.tsv', sep = '\t', header = TRUE)
length(which(!complete.cases(train)))
train$text_a[1:3]
train$text_a = as.character(train$text_a)
train$text_a = tm::removePunctuation(train$text_a)
train$text_a = tm::removeWords(x = train$text_a, stopwords(kind = "SMART"))
train$text_a[1:3]
train$text_a = tm::stripWhitespace(train$text_a)
train$text_a[1:3]
train$text_a = as.character(train$text_a)
train$text_a = tm::removePunctuation(train$text_a)
train$text_a = tm::removeWords(x = train$text_a, stopwords(kind = "SMART"))
train$text_a = tm::stripWhitespace(train$text_a)
train$text_a[1:3]
train$text_a = str_replace_all(train$text_a, "[^[:alnum:],[:blank:]/\\-]", "")
train$text_a[1:3]
# We are going to use several packages.
# Make sure that the packages are installed.
#
install.packages(c("tm", "SnowballC", "wordcloud", "proxy", "kernlab", "NLP", "openNLP"))
library(ggplot2)
# Load the classic TIME Magazine article collection consisting of 425 articles from the TIME Magazine of the 1960's
conn = file("time.mag.txt", open="r")
text = readLines(conn)
close(conn)
corpus <- Corpus(VectorSource(train$text_a)) # turn into corpus
corpus <- TermDocumentMatrix(corpus)
corpus <- Corpus(VectorSource(train$text_a)) # turn into corpus
tdm <- TermDocumentMatrix(corpus)
tdm
rownames(tdm)
tdm
mat <- as.matrix(tdm)
mat
wordFreq <- sort(rowSums(mat), decreasing=TRUE)
wordFreq
setwd("~/Documents/IS/lab8")
library(ggplot2)
# Load the classic TIME Magazine article collection consisting of 425 articles from the TIME Magazine of the 1960's
conn = file("time.mag.txt", open="r")
text = readLines(conn)
close(conn)
length(text)
head(text)
# Load a framework for text mining applications within R.
library(tm)
# Construct a corpus for a vector as input.
corpus <- Corpus(VectorSource(text))
length(corpus)
content(corpus[[1]])
# Change letters to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove punctuations
corpus <- tm_map(corpus, removePunctuation)
# Remove numbers
corpus <- tm_map(corpus, removeNumbers)
# Remove stopwords (these are some of the most common, short function words, such as the, is, at, which, etc.)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
# Read the custom stopwords list
conn = file("english.stop.txt", open="r")
mystopwords = readLines(conn)
close(conn)
# Inspect our custom stopwords list
mystopwords
# Remove stopwords
corpus <- tm_map(corpus, removeWords, mystopwords)
# Stem words to retrieve their radicals, so that various forms derived from a stem would be taken as the same
# when counting word frequency
corpus <- tm_map(corpus, stemDocument)
# Strip extra whitespace from text documents
corpus <- tm_map(corpus, stripWhitespace)
# Have a look at the first document in the corpus
content(corpus[[1]])
tdm <- TermDocumentMatrix(corpus)
# The term-document matrix is composed of 14557 terms and 425 documents.
# It is very sparse, with 99% of the entries being zero.
tdm
# Retrieve the list of terms
rownames(tdm)
# Find the term "moscow"
idx <- which(rownames(tdm) == "moscow")
idx
# Have a look at the term "moscow" in the documents
inspect(tdm[idx,])
# Inspect frequent words (with frequency no less than 300)
findFreqTerms(tdm, lowfreq=300)
# The plot shows the most frequent words in the corpus
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency >= 300)
qplot(seq(length(termFrequency)),sort(termFrequency), xlab = "index", ylab = "Freq")
# A word cloud is a visual representation for text data.
# Tags are single words, and the importance of each tag is shown with font size and color.
library(wordcloud)
# Convert the term-document matrix to a normal matrix and calculate word frequencies
mat <- as.matrix(tdm)
wordFreq <- sort(rowSums(mat), decreasing=TRUE)
grayLevels <- gray((wordFreq+10) / (max(wordFreq)+10))
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=100, random.order=F, colors=grayLevels)
wordFreq
grayLevels
wordFreq
setwd("~/Documents/IS/hw02/IntelligentSystems")
corpus <- Corpus(VectorSource(train$text_a)) # turn into corpus
tdm <- TermDocumentMatrix(corpus)
rownames(tdm)
tdm
findFreqTerms(tdm, lowfreq=300)
findFreqTerms(tdm, lowfreq=200)
findFreqTerms(tdm, lowfreq=150)
findFreqTerms(tdm, lowfreq=100)
findFreqTerms(tdm, lowfreq=80)
findFreqTerms(tdm, lowfreq=50)
findFreqTerms(tdm, lowfreq=50)
termFrequency <- rowSums(as.matrix(tdm))
termFrequenct
termFrequency
termFrequency <- rowSums(as.matrix(tdm), decreasing=TRUE)
termFrequency <- rowSums(as.matrix(tdm), decreasing=TRUE)
termFrequency <- rowSums(as.matrix(tdm))
wordFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
wordFreq
termFrequency <- subset(termFrequency, termFrequency >= 50)
qplot(seq(length(termFrequency)),sort(termFrequency), xlab = "index", ylab = "Freq")
wordFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
qplot(seq(length(termFrequency)),sort(termFrequency), xlab = "index", ylab = "Freq")
wordFreq
wordFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
qplot(seq(length(termFrequency)),sort(termFrequency), xlab = "index", ylab = "Frequency")
qplot(seq(length(wordFreq)),sort(wordFreq), xlab = "index", ylab = "Frequency")
mostFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
mostFreq <- subset(mostFreq, mostFreq >= 50)
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")
qplot(seq(length(wordFreq)),sort(wordFreq), xlab = "index", ylab = "Frequency")
mostFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency", colour = cyl)
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency", colour = runif(10))
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")
qplot(seq(length(wordFreq)),sort(wordFreq), xlab = "index", ylab = "Frequency")
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")
findFreqTerms(tdm, lowfreq=50)
mostFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
mostFreq <- subset(mostFreq, mostFreq >= 50)
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")
leastFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
leastFreq
leastFreq <- sort(rowSums(as.matrix(tdm)), decreasing=FALSE)
leastFreq
wordFreq
wordFreq[wordFreq = 1]
wordFreq[1]
wordFreq[1,]
wordFreq
wordFreq == 1
wordFreq[wordFreq==1]
wordFreq[which(wordFreq==1)]
length(wordFreq[which(wordFreq==1)])
length(wordFreq[which(wordFreq==1)])
length(wordFreq[which(wordFreq<10)])
length(wordFreq[which(wordFreq<50)])
length(wordFreq[which(wordFreq==1)])
length(wordFreq[which(wordFreq<10)])
length(wordFreq[which(wordFreq<50)])
length(wordFreq[which(wordFreq==1)])
length(wordFreq[which(wordFreq<10)])
length(wordFreq[which(wordFreq<50)])
length(wordFreq[which(wordFreq<5)])
length(wordFreq[which(wordFreq<10)])
length(wordFreq[which(wordFreq<50)])
length(wordFreq[which(wordFreq<100)])
length(wordFreq)
length(wordFreq[which(wordFreq<10)])
length(wordFreq[which(wordFreq<5)])
length(wordFreq[which(wordFreq==1)])
length(wordFreq[which(wordFreq==2)])
length(wordFreq[which(wordFreq==1)])
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")
qplot(seq(length(wordFreq)),sort(wordFreq), xlab = "index", ylab = "Frequency")
length(wordFreq[which(wordFreq==1)])
length(wordFreq)
length(wordFreq[which(wordFreq==1)])
wordFreq
summary(wordFreq)
type(wordFreq)
wordFreq
wordFreq[,1]
wordFreq[,2]
wordFreq[,0]
wordFreq[0]
wordFreq()
wordFreq(1,)
wordFreq(1)
wordFreq[,]
wordFreq[1,]
wordFreq[1,2]
wordFreq
wordFreq[2]
wordFreq[1]
wordFreq[1,2]
wordFreq[1]
wordFreq[4]
wordFreq[4,]
wordFreq[4]
wordFreq[,4]
wordFreq[4]
as.matrix(tdm)
freq <- seq(mostFreq)
freq
freq <- (mostFreq)
freq
rowSums(as.matrix(tdm)
klnlasd
rowSums(as.matrix(tdm)0
rowSums(as.matrix(tdm)0
rowSums(as.matrix(tdm))
rowSums(as.matrix(tdm))[1]
rowSums(as.matrix(tdm))[1][1]
rowSums(as.matrix(tdm))[1][2]
rowSums(as.matrix(tdm))[1][1]
rowSums(as.matrix(tdm))[1,1]
rowSums(as.matrix(tdm))
rowSums(as.matrix(tdm))[1] + rowSums(as.matrix(tdm))[1]
rowSums(as.matrix(tdm))[1]
rowSums(as.matrix(tdm))[1] + rowSums(as.matrix(tdm))[2]
rowSums(as.matrix(tdm))[2]
unique(wordFreq)
freq <- sort(unique(wordFreq), decreasing=TRUE)
freq
freq <- sort(unique(wordFreq), decreasing=FALSE)
freq
wordFreq[wordFreq==freq]
wordFreq[which(wordFreq==freq)]
occ <- wordFreq[wordFreq == freq]
occ <- wordFreq == freq
occ <- wordFreq == freq
occ <- wordFreq == 1
occ
occ <- wordFreq[wordFreq == 1]
occ
occ <- length(wordFreq[wordFreq == 1])
occ
length(wordFreq[which(wordFreq==1)])
length(wordFreq[wordFreq<5])
length(wordFreq[wordFreq<10])
freq <- sort(unique(wordFreq), decreasing=FALSE)
occ <- length(wordFreq[wordFreq == 1])
occ
for (i in 1:length(freq)) {
occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
occ
freq
qplot(freq, occ, xlab = "Frequency of word", ylab = "# of occurencies")
corpus <- Corpus(VectorSource(train$text_a)) # turn into corpus
wordFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
qplot(seq(length(wordFreq)),sort(wordFreq), xlab = "index", ylab = "Frequency")
findFreqTerms(tdm, lowfreq=50)
mostFreq <- subset(wordFreq, wordFreq >= 50)
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")
length(wordFreq)
length(wordFreq[wordFreq<10])
length(wordFreq[wordFreq<5])
length(wordFreq[wordFreq==1])
freq <- sort(unique(wordFreq), decreasing=FALSE)
for (i in 1:length(freq)) {
occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
qplot(freq, occ, xlab = "Frequency of word", ylab = "# of occurencies")
qplot(freq[1:10], occ[1:10], xlab = "Frequency of word", ylab = "# of occurencies")
qplot(freq[1:10], occ[1:10], xlab = "Frequency of word", ylab = "# of occurencies", colours="red")
qplot(freq[1:10], occ[1:10], xlab = "Frequency of word", ylab = "# of occurencies", colour="red")
qplot(freq[1:20], occ[1:20], xlab = "Frequency of word", ylab = "# of occurencies", colour="red")
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies", colour="red")
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies", colour="yellow")
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies", colour="red")
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies", colour='blue')
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies")
occ <- vector()
for (i in 1:length(freq)) {
occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies")
corpus <- Corpus(VectorSource(train$text_a)) # turn into corpus
stemDocument(corpus, language = "english")
stemmed <- stemDocument(train$text_a, language = "english")
corpus <- Corpus(VectorSource(stemmed)) # turn into corpus
tdm <- TermDocumentMatrix(corpus)
wordFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
qplot(seq(length(wordFreq)),sort(wordFreq), xlab = "index", ylab = "Frequency")
corpus2 <- Corpus(VectorSource(stemmed)) # turn into corpus
wordFreq <- sort(rowSums(as.matrix(tdm2)), decreasing=TRUE)
tdm2 <- TermDocumentMatrix(corpus2)
wordFreq <- sort(rowSums(as.matrix(tdm2)), decreasing=TRUE)
qplot(seq(length(wordFreq)),sort(wordFreq), xlab = "index", ylab = "Frequency")
findFreqTerms(tdm2, lowfreq=50)
mostFreq <- subset(wordFreq, wordFreq >= 50)
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")
length(wordFreq)
length(wordFreq[wordFreq<10])
length(wordFreq[wordFreq<5])
length(wordFreq[wordFreq==1])
freq <- sort(unique(wordFreq), decreasing=FALSE)
occ <- vector()
for (i in 1:length(freq)) {
occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies")
k <- 2
kmeansResult <- kmeans(mat, k)
# Find the most popular words in every cluster
for (i in 1:k)
# Find the most popular words in every cluster
for (i in 1:k) {
s <- sort(kmeansResult$centers[i,], decreasing=T)
cat(names(s)[1:10], "\n")
}
kmeansResult
summary(kmeansResult)
kmeansResult$centers[1,]
names(kmeansResult$centers[1,])
# Find the most popular words in every cluster
for (i in 1:k) {
s <- sort(kmeansResult$centers[i,], decreasing=TRUE)
cat(names(s)[1:10], "\n")
}
dtm
dtm
dtm <-DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf)) # TODO: corpus2
dtm
#train$text_a = str_replace_all(train$text_a, "[^[:alnum:],[:blank:]/\\-]", "")
train$text_a <- iconv(train$text_a, to='UTF-8', sub='byte')
train$text_a[1:3]
tdm <- tm::DocumentTermMatrix(corpus)
tdm.tfidf <- tm::weightTfIdf(tdm)
tdm.tfidf <- tm::removeSparseTerms(tdm.tfidf, 0.999)
?tm::removeSparseTerms
tdm.tfidf <- tm::removeSparseTerms(tdm.tfidf, 0.7)
tfidf.matrix <- as.matrix(tdm.tfidf)
dist.matrix = proxy::dist(tfidf.matrix, method = "cosine")
dist.matrix
k <- 2
kmeans <- kmeans(tfidf.matrix, k)
tdm <- tm::DocumentTermMatrix(corpus)
tfidf <- tm::weightTfIdf(tdm)
tfidf.matrix <- as.matrix(tdm.tfidf)
dist.matrix = proxy::dist(tfidf.matrix, method = "cosine")
k <- 2
kmeans <- kmeans(tfidf.matrix, k)
tdm <- tm::DocumentTermMatrix(corpus)
tfidf <- tm::weightTfIdf(tdm)
tfidf <- tm::removeSparseTerms(tfidf, 0.999)
k <- 2
kmeans <- kmeans(tfidf.matrix, k)
k <- 4
kmeans <- kmeans(tfidf.matrix, k)
k <- 8
kmeans <- kmeans(tfidf.matrix, k)
k <- 2
kmeans <- kmeans(tfidf.matrix, k)
# Find the most popular words in every cluster
for (i in 1:k) {
s <- sort(kmeansResult$centers[i,], decreasing=TRUE)
cat(names(s)[1:10], "\n")
}
k <- 2
# Find the most popular words in every cluster
for (i in 1:k) {
s <- sort(kmeans$centers[i,], decreasing=TRUE)
cat(names(s)[1:10], "\n")
}
kmeans <- kmeans(tfidf.matrix, k)
tdm <- tm::DocumentTermMatrix(corpus)
tdm <- tm::DocumentTermMatrix(corpus2)
tfidf <- tm::weightTfIdf(tdm)
tfidf <- tm::removeSparseTerms(tfidf, 0.8)
tfidf.matrix <- as.matrix(tdm.tfidf)
dist.matrix = proxy::dist(tfidf.matrix, method = "cosine")
k <- 2
kmeans <- kmeans(tfidf.matrix, k)
points <- cmdscale(dist.matrix, k = 2)
dist.matrix = proxy::dist(tfidf.matrix, method = "cosine")
k <- 2
kmeans <- kmeans(tfidf.matrix, k)
points <- cmdscale(dist.matrix, k = 2)
corpus
points <- cmdscale(dist.matrix, k = 2)
palette <- colorspace::diverge_hcl(truth.K) # Creating a color palette
palette <- colorspace::diverge_hcl(2) # Creating a color palette
previous.par <- par(mfrow=c(2,2), mar = rep(1.5, 4))
plot(points, main = 'K-Means clustering', col = as.factor(master.cluster),
mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0),
xaxt = 'n', yaxt = 'n', xlab = '', ylab = '')
tdm <- tm::DocumentTermMatrix(corpus)
tdm
tfidf <- tm::weightTfIdf(tdm)
tfidf
tfidf <- tm::removeSparseTerms(tfidf, 0.99)
tfidf
tfidf_matrix <- as.matrix(tdm.tfidf)
dist.matrix = proxy::dist(tfidf_matrix, method = "cosine")
k <- 2
kmeans <- kmeans(tfidf.matrix, k)
hierarchical <- hclust(dist_matrix, method = "ward.D2")
dist_matrix = proxy::dist(tfidf_matrix, method = "cosine")
k <- 2
kmeans <- kmeans(tfidf.matrix, k)
hierarchical <- hclust(dist_matrix, method = "ward.D2")
hierarchical <- hclust(dist_matrix, method = "ward.D2")
setwd("~/Documents/IS/lab8")
library(ggplot2)
# Load the classic TIME Magazine article collection consisting of 425 articles from the TIME Magazine of the 1960's
conn = file("time.mag.txt", open="r")
text = readLines(conn)
close(conn)
length(text)
head(text)
# Load a framework for text mining applications within R.
library(tm)
# Construct a corpus for a vector as input.
corpus <- Corpus(VectorSource(text))
length(corpus)
content(corpus[[1]])
# Change letters to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove punctuations
corpus <- tm_map(corpus, removePunctuation)
# Remove numbers
corpus <- tm_map(corpus, removeNumbers)
# Remove stopwords (these are some of the most common, short function words, such as the, is, at, which, etc.)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
# Read the custom stopwords list
conn = file("english.stop.txt", open="r")
mystopwords = readLines(conn)
close(conn)
# Inspect our custom stopwords list
mystopwords
# Remove stopwords
corpus <- tm_map(corpus, removeWords, mystopwords)
# Stem words to retrieve their radicals, so that various forms derived from a stem would be taken as the same
# when counting word frequency
corpus <- tm_map(corpus, stemDocument)
# Strip extra whitespace from text documents
corpus <- tm_map(corpus, stripWhitespace)
# Have a look at the first document in the corpus
content(corpus[[1]])
tdm <- TermDocumentMatrix(corpus)
# The term-document matrix is composed of 14557 terms and 425 documents.
# It is very sparse, with 99% of the entries being zero.
tdm
# Retrieve the list of terms
rownames(tdm)
# Find the term "moscow"
idx <- which(rownames(tdm) == "moscow")
idx
# Have a look at the term "moscow" in the documents
inspect(tdm[idx,])
# Inspect frequent words (with frequency no less than 300)
findFreqTerms(tdm, lowfreq=300)
# The plot shows the most frequent words in the corpus
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency >= 300)
qplot(seq(length(termFrequency)),sort(termFrequency), xlab = "index", ylab = "Freq")
# A word cloud is a visual representation for text data.
# Tags are single words, and the importance of each tag is shown with font size and color.
library(wordcloud)
# Convert the term-document matrix to a normal matrix and calculate word frequencies
mat <- as.matrix(tdm)
wordFreq <- sort(rowSums(mat), decreasing=TRUE)
grayLevels <- gray((wordFreq+10) / (max(wordFreq)+10))
wordcloud(words=names(wordFreq), freq=wordFreq, min.freq=100, random.order=F, colors=grayLevels)
# Constructs a document-term matrix
dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
mat <- as.matrix(dtm)
# Cluster the documents using the kmeans method with the number of clusters set to 3
k <- 3
kmeansResult <- kmeans(mat, k)
# Find the most popular words in every cluster
for (i in 1:k)
{
s <- sort(kmeansResult$centers[i,], decreasing=T)
cat(names(s)[1:10], "\n")
}
setwd("~/Documents/IS/hw02/IntelligentSystems")
tdm <- tm::DocumentTermMatrix(corpus)
tfidf <- tm::weightTfIdf(tdm)
tfidf_matrix <- as.matrix(tdm.tfidf)
dist_matrix = proxy::dist(tfidf_matrix, method = "cosine")
k <- 2
kmeans <- kmeans(tfidf.matrix, k)
dtm <- tm::DocumentTermMatrix(corpus)
tfidf <- tm::weightTfIdf(dtm)
tfidf_matrix <- as.matrix(tdm.tfidf)
dist_matrix = proxy::dist(tfidf_matrix, method = "cosine")
k <- 2
kmeans <- kmeans(tfidf.matrix, k)
#dtm <- tm::DocumentTermMatrix(corpus)
#tfidf <- tm::weightTfIdf(dtm)
dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
#tfidf <- tm::removeSparseTerms(tfidf, 0.99)
tfidf_matrix <- as.matrix(tdm)
dist_matrix = proxy::dist(tfidf_matrix, method = "cosine")
k <- 2
kmeans <- kmeans(tfidf.matrix, k)
#dtm <- tm::DocumentTermMatrix(corpus)
#tfidf <- tm::weightTfIdf(dtm)
dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
#tfidf <- tm::removeSparseTerms(tfidf, 0.99)
tfidf_matrix <- as.matrix(tdm)
k <- 2
kmeans <- kmeans(tfidf.matrix, k)
k <- 4
kmeans <- kmeans(tfidf.matrix, k)
