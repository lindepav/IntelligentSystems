genes <- read.csv("DLBCL.csv")
target <- genes$class
splited <- createDataPartition(
y = target,
p = .8,
list = FALSE
)
View(genes)
View(genes)
## Task 2: Genetic feature selection
# 1) Read in the dataset
```{r import, echo=F}
genes <- read.csv("DLBCL.csv")
target <- genes$class
splited <- createDataPartition(
y = target,
p = .8,
list = FALSE,
)
splited
learn = genes[ splited, ]
test = genes[-splited, ]
learn$X <- NULL
test$X <- NULL
learn
length(learn)
sample(learn)
length(sample(learn))
length(learn)
splited <- createDataPartition(
y = target,
p = .8,
list = FALSE,
)
learn = genes[ splited, ]
test = genes[-splited, ]
learn$X <- NULL
test$X <- NULL
learn = sample(learn)
learn = learn[1:1000]
splited <- createDataPartition(
y = target,
p = .8,
list = FALSE,
)
test = genes[-splited, ]
test = genes[-splited, ]
learn$X <- NULL
test = genes[-splited, ]
learn$X <- NULL
test$X <- NULL
learn = learn[1:1000]
length(learn)
ctrl <- trainControl(number = 3,
method = "repeatedcv")
PLSModel <- train(
class ~ .,
data = learn,
method = "pls",
preProc = c("center", "scale"),
trControl = ctrl,
tuneLenght = 1000
)
PLSModel <- train(
class ~ .,
data = learn,
method = "pls",
preProc = c("center", "scale"),
trControl = ctrl,
tuneLenght = 1000
)
RFModel <- train(
class ~ .,
data = learn,
method = "rf",
preProc = c("center", "scale"),
trControl = ctrl,
tuneLenght = 1000
)
PLSModel <- train(
class ~ .,
data = learn,
method = "pls",
preProc = c("center", "scale"),
trControl = ctrl,
tuneLenght = 1000
)
RFModel <- train(
class ~ .,
data = learn,
method = "rf",
preProc = c("center", "scale"),
trControl = ctrl,
tuneLenght = 1000
)
xgbModel <- train(
class ~ .,
data = learn,
method = "xgbDART",
preProc = c("center", "scale"),
trControl = ctrl,
tuneLenght = 100
)
SVMModel <- train(
class ~ .,
data = learn,
method = "svmRadial",
preProc = c("center", "scale"),
trControl = ctrl,
tuneLenght = 1000
)
models_compare <- resamples(list(RF=RFModel, XGBDART=xgbModel, PLS=PLSModel, SVM=SVMModel))
summary(models_compare)
varimp <- varImp(RFModel)
imp <- varimp$importance
plot(varimp, top = 15)
plsClasses <- predict(xgbModel, newdata = test)
str(plsClasses)
plsProbs <- predict(xgbModel, newdata = test, type = "prob")
head(plsProbs)
langth(test)
length(test)
confusionMatrix(data = plsClasses, test$class)
control <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
number = 3,
verbose = FALSE)
outcomeName <- 'class'
predictors <- names(learn)[!names(learn) %in% outcomeName]
results <- rfe(learn[,predictors], learn[,outcomeName], rfeControl=control,
sizes = c(1, 2, 4, 6, 8, 12, 16, 20, 32, 64, 128, 256, 512, 1000))
results <- rfe(learn[,predictors], learn[,outcomeName], rfeControl=control,
sizes = c(1, 2, 4, 6, 8, 12, 16, 20, 32, 64, 128, 256, 512, 1000))
results
predictors(results)
plot(results)
result <- genes[, predictors(results)]
topCor <- data.frame(imp)
topCor$Vars <- row.names(topCor)
topCor <- topCor[order(-topCor$Overall),][1:20,]$Vars
cors <- genes[, topCor]
correlation <- head(round(cor(result, cors),2))
correlation
corrplot(correlation, method="color")
correlation
corrplot(correlation, method="color")
summary(models_compare)
varimp <- varImp(PLSModel)
imp <- varimp$importance
varimp <- varImp(PLSModel)
imp <- varimp$importance
plot(varimp, top = 15)
plsClasses <- predict(xgbModel, newdata = test)
str(plsClasses)
plsProbs <- predict(xgbModel, newdata = test, type = "prob")
head(plsProbs)
test
test
dim(test)
plsProbs
plsProbs
confusionMatrix(data = plsClasses, test$class)
RF
RF
RFModel
RFModel$results
RFModel$finalModel
control <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
number = 3,
verbose = FALSE)
outcomeName <- 'class'
predictors <- names(learn)[!names(learn) %in% outcomeName]
results <- rfe(learn[,predictors], learn[,outcomeName], rfeControl=control,
sizes = c(1, 2, 4, 6, 8, 12, 16, 20, 32, 64, 128, 256, 512, 1000))
results
predictors(results)
plot(results)
predictors
learn[,predictors]
# BONUS: Plot the trace of evolution
history_x = c()
history_y = c()
for (i in 1:12) {
GA2 <- ga(type = "real-valued",
fitness =  function(x) f(x[1], x[2]),
lower = c(-1, -1), upper = c(1, 1),
popSize = 50, maxiter = 50*i, run = 100)
history_x[i] <- GA2@solution[,1]
history_y[i] <- GA2@solution[,2]
}
results
results
results$results
results$result
results
predict(results)
predict(results, newdata = test)
predict(results, newdata = test, type = "prob")
predict(results, newdata = test, type = "prob")
plsProbs
results
results$bestSubset
results$pred
results$variables
result <- genes[, predictors(results)]
result
genes[, results$variables]
results$variables
results$optVariables
genes[, results$Optvariables]
predictors(results)
genes[, results$Optvariables]
genes[, predictors(results)]
predictors(results)
results$Optvariables
results$Optvariables
results$optVariables
genes[, results$optVariables]
#result <- genes[, predictors(results)]
result <- genes[, results$optVariables]
topCor <- data.frame(imp)
topCor$Vars <- row.names(topCor)
topCor <- topCor[order(-topCor$Overall),][1:20,]$Vars
cors <- genes[, topCor]
correlation <- head(round(cor(result, cors),2))
correlation
corrplot(correlation, method="color")
corrplot(correlation, method="color")
#result <- genes[, predictors(results)]
result <- genes[, results$optVariables]
topCor <- data.frame(imp)
topCor$Vars <- row.names(topCor)
topCor <- topCor[order(-topCor$Overall),][1:20,]$Vars
cors <- genes[, topCor]
correlation <- head(round(cor(result, cors),2))
correlation
corrplot(correlation, method="color")
result <- genes[, predictors(results)]
#result <- genes[, results$optVariables]
topCor <- data.frame(imp)
topCor$Vars <- row.names(topCor)
topCor <- topCor[order(-topCor$Overall),][1:20,]$Vars
cors <- genes[, topCor]
correlation <- head(round(cor(result, cors),2))
correlation
corrplot(correlation, method="color")
result <- genes[, predictors(results)]
topCor <- data.frame(imp)
topCor$Vars <- row.names(topCor)
topCor <- topCor[order(-topCor$Overall),][1:20,]$Vars
correlation <- head(round(cor(result, cors),2))
correlation
corrplot(correlation, method="color")
predictors <- names(learn)[!names(learn) %in% outcomeName]
result <- genes[, predictors(results)]
topCor <- data.frame(imp)
topCor$Vars <- row.names(topCor)
topCor <- topCor[order(-topCor$Overall),][1:20,]$Vars
cors <- genes[, topCor]
correlation <- head(round(cor(result, cors),2))
correlation
corrplot(correlation, method="color")
result <- genes[, predictors(results)]
models_compare
topCor
summary(results)
predictors(results)
plot(results)
### Compare the final result (set of features) with the same number of features, that correlate the most with the target variable
```{r cor, echo=TRUE}
result <- genes[, predictors(results)]
topCor <- data.frame(imp)
topCor$Vars <- row.names(topCor)
topCor <- topCor[order(-topCor$Overall),][1:20,]$Vars
cors <- genes[, topCor]
correlation <- head(round(cor(result, cors),2))
corrplot(correlation, method="color")
```
plot(results)
library(corrplot)
### Compare the final result (set of features) with the same number of features, that correlate the most with the target variable
```{r cor, echo=TRUE}
result <- genes[, predictors(results)]
topCor <- data.frame(imp)
topCor$Vars <- row.names(topCor)
topCor <- topCor[order(-topCor$Overall),][1:20,]$Vars
cors <- genes[, topCor]
correlation <- head(round(cor(result, cors),2))
corrplot(correlation, method="color")
```
install.packages(c("gtools", "tm"))
knitr::opts_chunk$set(echo = TRUE)
library(gtools) # if problems calling library, install.packages("gtools", dependencies = T)
test = read.table(file = 'test.tsv', sep = '\t', header = TRUE)
library(gtools) # if problems calling library, install.packages("gtools", dependencies = T)
#library(qdap) # qualitative data analysis package (it masks %>%)
library(stringr)
library(tm) # framework for text mining; it loads NLP package
library(ggplot2)
library(gtools) # if problems calling library, install.packages("gtools", dependencies = T)
#library(qdap) # qualitative data analysis package (it masks %>%)
library(stringr)
library(tm) # framework for text mining; it loads NLP package
library(ggplot2)
## 0. Read input
```{r import, echo=TRUE}
train = read.table(file = 'train.tsv', sep = '\t', header = TRUE, stringsAsFactors = FALSE)
test = read.table(file = 'test.tsv', sep = '\t', header = TRUE)
length(which(!complete.cases(train)))
train$text_a[1:3]
```
setwd("C:/Users/miner/Desktop/IntelligentSystems")
## 0. Read input
```{r import, echo=TRUE}
train = read.table(file = 'train.tsv', sep = '\t', header = TRUE, stringsAsFactors = FALSE)
test = read.table(file = 'test.tsv', sep = '\t', header = TRUE)
length(which(!complete.cases(train)))
train$text_a[1:3]
```
# 1. Cleaning data
## Remove punctuation and stopwords (?TODO: tolower)
```{r punctuation, echo=TRUE}
train$text_a = as.character(train$text_a)
train$text_a = tm::removePunctuation(train$text_a)
train$text_a = tm::removeWords(x = train$text_a, stopwords(kind = "SMART"))
train$text_a = tm::stripWhitespace(train$text_a)
train$text_a[1:3]
```
## Anonymize proper nouns
```{r anonymize, echo=TRUE}
## Remove unknown symbols (non UTF-8 characters)
```{r symbols, echo=TRUE}
#train$text_a = str_replace_all(train$text_a, "[^[:alnum:],[:blank:]/\\-]", "")
train$text_a <- iconv(train$text_a, to='UTF-8', sub='byte')
train$text_a[1:3]
```
# 2. Exploration
### I. Plot the frequency of words (without stemmization)
```{r frequency1, echo=TRUE}
corpus <- Corpus(VectorSource(train$text_a)) # turn into corpus
tdm <- TermDocumentMatrix(corpus)
wordFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
qplot(seq(length(wordFreq)),sort(wordFreq), xlab = "index", ylab = "Frequency")
findFreqTerms(tdm, lowfreq=50)
mostFreq <- subset(wordFreq, wordFreq >= 50)
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")
length(wordFreq)
length(wordFreq[wordFreq<10])
length(wordFreq[wordFreq<5])
length(wordFreq[wordFreq==1])
freq <- sort(unique(wordFreq), decreasing=FALSE)
occ <- vector()
for (i in 1:length(freq)) {
occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies")
### I. Plot the frequency of words (with stemmization)
```{r stemmization1, echo=TRUE}
stemmed <- stemDocument(train$text_a, language = "english")
corpus2 <- Corpus(VectorSource(stemmed)) # turn into corpus
```
```{r freq2, echo=FALSE}
tdm2 <- TermDocumentMatrix(corpus2)
wordFreq <- sort(rowSums(as.matrix(tdm2)), decreasing=TRUE)
mostFreq <- subset(wordFreq, wordFreq >= 50)
```
```{r stemmization2, echo=TRUE}
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")
stemmed <- stemDocument(train$text_a, language = "english")
### I. Plot the frequency of words (with stemmization)
```{r stemmization1, echo=TRUE}
stemmed <- stemDocument(train$text_a, language = "english")
```{r setup, include=FALSE}
---
title: "Assigment 2"
author: "Pavel Linder, Nikita Brancatisano"
date: "12/26/2019"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
```{r libraries, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
library(gtools) # if problems calling library, install.packages("gtools", dependencies = T)
#library(qdap) # qualitative data analysis package (it masks %>%)
library(stringr)
library(tm) # framework for text mining; it loads NLP package
library(ggplot2)
# 2. Exploration
### I. Plot the frequency of words (without stemmization)
```{r frequency1, echo=TRUE}
corpus <- Corpus(VectorSource(train$text_a)) # turn into corpus
### I. Plot the frequency of words (with stemmization)
```{r stemmization1, echo=TRUE}
stemmed <- stemDocument(train$text_a, language = "english")
library(SnowballC)
install.packages('SnowballC')
library(SnowballC)
stemmed <- stemDocument(train$text_a, language = "english")
corpus2 <- Corpus(VectorSource(stemmed)) # turn into corpus
stemmed <- stemDocument(train$text_a, language = "english")
corpus2 <- Corpus(VectorSource(stemmed)) # turn into corpus
```{r freq2, echo=FALSE}
tdm2 <- TermDocumentMatrix(corpus2)
wordFreq <- sort(rowSums(as.matrix(tdm2)), decreasing=TRUE)
mostFreq <- subset(wordFreq, wordFreq >= 50)
tdm2 <- TermDocumentMatrix(corpus2)
wordFreq <- sort(rowSums(as.matrix(tdm2)), decreasing=TRUE)
mostFreq <- subset(wordFreq, wordFreq >= 50)
```{r stemmization2, echo=TRUE}
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")
length(wordFreq)
length(wordFreq[wordFreq<10])
length(wordFreq[wordFreq<5])
length(wordFreq[wordFreq==1])
length(wordFreq[wordFreq==1])
```{r freq3, echo=FALSE}
freq <- sort(unique(wordFreq), decreasing=FALSE)
occ <- vector()
for (i in 1:length(freq)) {
occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
freq <- sort(unique(wordFreq), decreasing=FALSE)
occ <- vector()
for (i in 1:length(freq)) {
occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
```{r stemmization3, echo=TRUE}
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies")
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies")
## II. Perform a clustering on the vectorized document space
We will use Weighted TF-IDF as a way to represent the document space:
```{r cluster, echo=TRUE}
tfidf <- as.matrix(tm::DocumentTermMatrix(corpus), control = list(weighting=weightTfIdf))
tfidf <- tm::removeSparseTerms(tfidf, 0.7)
k <- c(2, 4, 8, 16)
for (j in 1:4) {
kmeansResult <- kmeans(tfidf, k[j])
# Find the most popular words in every cluster
cat("for k = ", k[j], ":\n")
for (i in 1:k[j]) {
s <- sort(kmeansResult$centers[i,], decreasing=TRUE)
cat(names(s)[1:10], "\n")
}
cat("\n")
}
install.packages('tsne')
library(tsne)
ecb = function(x, y){ plot(x, t='n'); text(x, labels=trn[,65], col=cols[trn[,65] +1]); }
ecb = function(x, y){ plot(x, t='n'); text(x, labels=s, col=cols[s+1]); }
ecb = function(x, y){ plot(x, t='n'); text(x, labels=s[,65], col=cols[s[,65] +1]); }
tsne_res = tsne(s[,1:64], epoch_callback = ecb, perplexity=50, epoch=50)
s
plot(s[,16:22])
plot(s)
pc <- princomp(s[,1:12143], cor=TRUE, scores=TRUE)
pc <- princomp(s[,1:12142], cor=TRUE, scores=TRUE)
dim(s)
s <- sort(kmeansResult$centers[i,], decreasing=TRUE)
dim(s)
kmeansResult <- kmeans(tfidf, k[j])
kmeansResult <- kmeans(tfidf, k[j])
s <- sort(kmeansResult$centers[i,], decreasing=TRUE)
s <- sort(kmeansResult$centers[i,], decreasing=TRUE)
dim(s)
```{r cluster, echo=TRUE}
## II. Perform a clustering on the vectorized document space
We will use Weighted TF-IDF as a way to represent the document space:
```{r cluster, echo=TRUE}
tfidf <- as.matrix(tm::DocumentTermMatrix(corpus), control = list(weighting=weightTfIdf))
tfidf <- tm::removeSparseTerms(tfidf, 0.7)
k <- c(2, 4, 8, 16)
for (j in 1:4) {
kmeansResult <- kmeans(tfidf, k[j])
# Find the most popular words in every cluster
cat("for k = ", k[j], ":\n")
for (i in 1:k[j]) {
s <- sort(kmeansResult$centers[i,], decreasing=TRUE)
dim(s)
pc <- princomp(s[,1:12142], cor=TRUE, scores=TRUE)
cat(names(s)[1:10], "\n")
}
cat("\n")
}
kmeansResult <- kmeans(tfidf, k[j])
kmeansResult
pc <- princomp(s, cor=TRUE, scores=TRUE)
summary(pc)
s
pc <- princomp(s[,1], cor=TRUE, scores=TRUE)
summary(ttrain)
summary(train)
train
View(train)
View(train)
trainModel <- train
trainModel <- train
k <- c(2, 4, 8, 16)
for (j in 1:4) {
kmeansResult <- kmeans(trainModel, k[j])
str(kmeansResult)
}
trainModel <- train
k <- c(2, 4, 8, 16)
for (j in 1:4) {
kmeansResult <- kmeans(trainModel[,1:2], k[j])
str(kmeansResult)
}
kmeansResult <- kmeans(tfidf, k[j])
View(kmeansResult)
biplot(kmeansResult)
