x <- seq(-10, 10, length= 100)
y <- x
f <- function(x, y) {
x + y + 2*(x^2) + 2*x*y + 2*(y^2)
}
z <- outer(x, y, f)
z
persp(x,y,z)
#Find the solution to the following problem:
#Minimize:  x + y + 2x^2 + 2xy + 2y^2
#x - y <= 3
#x + 2y >= 6
#3x - 4y = 10
#4 <= x <= 20
#2 <= y <= 22
x <- seq(-100, 100, length= 100)
y <- x
f <- function(x, y) {
x + y + 2*(x^2) + 2*x*y + 2*(y^2)
}
z <- outer(x, y, f)
persp(x,y,z)
#Find the solution to the following problem:
#Minimize:  x + y + 2x^2 + 2xy + 2y^2
#x - y <= 3
#x + 2y >= 6
#3x - 4y = 10
#4 <= x <= 20
#2 <= y <= 22
x <- seq(-100, 100, length= 30)
y <- x
f <- function(x, y) {
x + y + 2*(x^2) + 2*x*y + 2*(y^2)
}
z <- outer(x, y, f)
persp(x,y,z)
Dmat <- 2*matrix(c(4, 2,
2, 4), ncol = 2, byrow = TRUE)
dvec <- -c(1, 1)
#constraints
Amat <- matrix(c(1, 2,
3, -4,
-1, 1,
1, 0,
-1, 0,
0, 1,
0, -1,
), ncol = 2, byrow = TRUE)
Amat <- t(Amat)
#constraints
Amat <- matrix(c(1, 2,
3, -4,
-1, 1,
1, 0,
-1, 0,
0, 1,
0, -1
), ncol = 2, byrow = TRUE)
Amat <- t(Amat)
bvec <- c(6, 10, -3, 4, -20, 2, -22)
res <- solve.QP(Dmat, dvec, Amat, bvec, meq = 1)
Amat
bvec
knitr::opts_chunk$set(echo = TRUE)
install.packages("gtools", dependencies = T)
library(gtools) # if problems calling library, install.packages("gtools", dependencies = T)
library(qdap) # qualitative data analysis package (it masks %>%)
library(tm) # framework for text mining; it loads NLP package
library(Rgraphviz) # depict the terms within the tm package framework
library(SnowballC); library(RWeka); library(rJava); library(RWekajars)  # wordStem is masked from SnowballC
library(Rstem) # stemming terms as a link from R to Snowball C stemmer
install.packages("gtools", dependencies = T)
install.packages("gtools", dependencies = T)
library(gtools) # if problems calling library, install.packages("gtools", dependencies = T)
knitr::opts_chunk$set(echo = TRUE)
library(qdap) # qualitative data analysis package (it masks %>%)
library(tm) # framework for text mining; it loads NLP package
library(Rgraphviz) # depict the terms within the tm package framework
library(SnowballC); library(RWeka); library(rJava); library(RWekajars)  # wordStem is masked from SnowballC
library(Rstem) # stemming terms as a link from R to Snowball C stemmer
install.packages(c("qdap", "RWekajars"))
library(qdap) # qualitative data analysis package (it masks %>%)
install.packages("gtools", dependencies = T)
install.packages("gtools", dependencies = T)
library(gtools) # if problems calling library, install.packages("gtools", dependencies = T)
library(qdap) # qualitative data analysis package (it masks %>%)
library(tm) # framework for text mining; it loads NLP package
library(Rgraphviz) # depict the terms within the tm package framework
install.packages("Rgraphviz")
install.packages("Rgraphviz", dependencies = T)
train$text_1
train$text_a
train = read.table(file = 'train.tsv', sep = '\t', header = TRUE, stringsAsFactors = FALSE)
test = read.table(file = 'test.tsv', sep = '\t', header = TRUE)
length(which(!complete.cases(learn)))
View(train)
train$text_a = as.character(train$text_a)
View(train)
train$text_a = tm::removePunctuation(train$text_a)
train$text_a = tm::removeWords(x = train$text_a, stopwords(kind = "SMART"))
train$text_a = str_replace_all(train$text_a, "  ", "")
train$text_a = tm::stripWhitespace(train$text_a)
library(tm) # framework for text mining; it loads NLP package
library(ggplot2)
length(which(!complete.cases(train)))
train$text_a[[1]]
train$text_a
train = read.table(file = 'train.tsv', sep = '\t', header = TRUE, stringsAsFactors = FALSE)
test = read.table(file = 'test.tsv', sep = '\t', header = TRUE)
length(which(!complete.cases(train)))
train$text_a[
train$text_a[1,10]
train$text_a[1,10]
train$text_a[[1,10]]
train$text_a[1:3]
train$text_a = str_replace_all(train$text_a, "[^[:alnum:]]", " ")
install.packages('stringr')
install.packages("stringr")
train$text_a = str_replace_all(train$text_a, "[^[:alnum:],[:blank:]/\\-]", "")
#library(qdap) # qualitative data analysis package (it masks %>%)
library(stringr)
train$text_a = str_replace_all(train$text_a, "[^[:alnum:],[:blank:]/\\-]", "")
train$text_a[1:3]
setwd("~/Documents/IS/hw02/IntelligentSystems")
knitr::opts_chunk$set(echo = TRUE)
library(gtools) # if problems calling library, install.packages("gtools", dependencies = T)
#library(qdap) # qualitative data analysis package (it masks %>%)
library(stringr)
library(tm) # framework for text mining; it loads NLP package
library(ggplot2)
train = read.table(file = 'train.tsv', sep = '\t', header = TRUE, stringsAsFactors = FALSE)
test = read.table(file = 'test.tsv', sep = '\t', header = TRUE)
length(which(!complete.cases(train)))
train$text_a[1:3]
train$text_a = as.character(train$text_a)
train$text_a = tm::removePunctuation(train$text_a)
train$text_a = tm::removePunctuation(train$text_a)
train$text_a = tm::removeWords(x = train$text_a, stopwords(kind = "SMART"))
train$text_a = tm::stripWhitespace(train$text_a)
train$text_a = tm::removePunctuation(train$text_a)
train$text_a = tm::removeWords(x = train$text_a, stopwords(kind = "SMART"))
train$text_a = tm::stripWhitespace(train$text_a)
train$text_a[1:3]
train$text_a = as.character(train$text_a)
train$text_a = tm::removePunctuation(train$text_a)
train$text_a = tm::removeWords(x = train$text_a, stopwords(kind = "SMART"))
train$text_a = tm::stripWhitespace(train$text_a)
train$text_a[1:3]
## Anonymize proper nouns
```{r anonymize, echo=TRUE}
## Remove unknown symbols (non UTF-8 characters)
```{r symbols, echo=TRUE}
#train$text_a = str_replace_all(train$text_a, "[^[:alnum:],[:blank:]/\\-]", "")
train$text_a <- iconv(train$text_a, to='UTF-8', sub='byte')
train$text_a[1:3]
```
# 2. Exploration
### I. Plot the frequency of words (without stemmization)
```{r frequency1, echo=TRUE}
corpus <- Corpus(VectorSource(train$text_a)) # turn into corpus
tdm <- TermDocumentMatrix(corpus)
wordFreq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
findFreqTerms(tdm, lowfreq=50)
mostFreq <- subset(wordFreq, wordFreq >= 50)
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")
qplot(seq(length(mostFreq)),sort(mostFreq), xlab = "index", ylab = "Frequency")
length(wordFreq)
length(wordFreq)
length(wordFreq[wordFreq<10])
length(wordFreq[wordFreq<5])
length(wordFreq[wordFreq<5])
length(wordFreq[wordFreq==1])
occ <- vector()
for (i in 1:length(freq)) {
occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies")
freq <- sort(unique(wordFreq), decreasing=FALSE)
occ <- vector()
for (i in 1:length(freq)) {
occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies")
stemmed <- stemDocument(train$text_a, language = "english")
corpus2 <- Corpus(VectorSource(stemmed)) # turn into corpus
stemmed <- stemDocument(train$text_a, language = "english")
corpus2 <- Corpus(VectorSource(stemmed)) # turn into corpus
tdm2 <- TermDocumentMatrix(corpus2)
wordFreq <- sort(rowSums(as.matrix(tdm2)), decreasing=TRUE)
mostFreq <- subset(wordFreq, wordFreq >= 50)
length(wordFreq[wordFreq<10])
length(wordFreq[wordFreq<5])
length(wordFreq[wordFreq==1])
length(wordFreq[wordFreq==1])
```{r freq3, echo=FALSE}
freq <- sort(unique(wordFreq), decreasing=FALSE)
occ <- vector()
for (i in 1:length(freq)) {
occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
freq <- sort(unique(wordFreq), decreasing=FALSE)
occ <- vector()
for (i in 1:length(freq)) {
occ[i] <- length(wordFreq[wordFreq == freq[i]])
}
```{r stemmization3, echo=TRUE}
qplot(freq[1:15], occ[1:15], xlab = "Frequency of word", ylab = "# of occurencies")
```{r cluster, echo=TRUE}
#dtm <- tm::DocumentTermMatrix(corpus)
#tfidf <- tm::weightTfIdf(dtm)
dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
#tfidf <- tm::removeSparseTerms(tfidf, 0.99)
tfidf_matrix <- as.matrix(tdm)
k <- 4
kmeans <- kmeans(tfidf.matrix, k)
#dtm <- tm::DocumentTermMatrix(corpus)
#tfidf <- tm::weightTfIdf(dtm)
dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
#tfidf <- tm::removeSparseTerms(tfidf, 0.99)
tfidf_matrix <- as.matrix(tdm)
dist_matrix = proxy::dist(tfidf_matrix, method = "cosine")
k <- 4
kmeans <- kmeans(tfidf_matrix, k)
dtm <- tm::DocumentTermMatrix(corpus)
tfidf <- tm::weightTfIdf(dtm)
#dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
#tfidf <- tm::removeSparseTerms(tfidf, 0.99)
tfidf_matrix <- as.matrix(tdm)
k <- 4
kmeans <- kmeans(tfidf_matrix, k)
points <- cmdscale(dist_matrix, k = 4)
dist_matrix = proxy::dist(tfidf_matrix, method = "cosine")
dist_matrix = proxy::dist(tfidf_matrix, method = "Euclidean")
plot(points, main = 'K-Means clustering', col = as.factor(master.cluster),
mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0),
xaxt = 'n', yaxt = 'n', xlab = '', ylab = '')
plot(points, main = 'K-Means clustering', col = as.factor(kmeans),
mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0),
xaxt = 'n', yaxt = 'n', xlab = '', ylab = '')
# Find the most popular words in every cluster
for (i in 1:k) {
s <- sort(kmeans$centers[i,], decreasing=TRUE)
cat(names(s)[1:10], "\n")
}
k <- 2
kmeans <- kmeans(tfidf_matrix, k)
kmeans <- kmeans(tfidf_matrix, k)
# Find the most popular words in every cluster
for (i in 1:k) {
s <- sort(kmeans$centers[i,], decreasing=TRUE)
cat(names(s)[1:10], "\n")
}
tfidf_matrix
tfidf_matrix[1,1]
tfidf_matrix[1,2]
dtm
dtm[1]
dtm[1,]
dtm[1,1]
dtm[1,1,1]
dtm[1,1,1,1]
s
kmeans
kmeans$cluster
kmeans$cluster[s]
# Find the most popular words in every cluster
for (i in 1:k) {
s <- sort(kmeans$centers[i,], decreasing=TRUE)
kmeans$cluster[s]
cat(names(s)[1:10], "\n")
}
kmeans$cluster[208]
kmeans$cluster[815]
kmeans$centers
kmeans$centers[1,]
names(kmeans$centers[1,])
dtm <- DocumentTermMatrix(corpus)
tfidf <- tm::weightTfIdf(dtm)
#dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
#tfidf <- tm::removeSparseTerms(tfidf, 0.99)
tfidf_matrix <- as.matrix(tdm)
k <- 2
kmeans <- kmeans(tfidf_matrix, k)
# Find the most popular words in every cluster
for (i in 1:k) {
s <- sort(kmeans$centers[i,], decreasing=TRUE)
kmeans$cluster[s]
cat(names(s)[1:10], "\n")
}
content(corpus[[1]])
tdm
dtm <- tm::DocumentTermMatrix(corpus2)
tfidf <- tm::weightTfIdf(dtm)
#dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
#tfidf <- tm::removeSparseTerms(tfidf, 0.99)
tfidf_matrix <- as.matrix(tdm)
k <- 2
kmeans <- kmeans(tfidf_matrix, k)
# Find the most popular words in every cluster
for (i in 1:k) {
s <- sort(kmeans$centers[i,], decreasing=TRUE)
kmeans$cluster[s]
cat(names(s)[1:10], "\n")
}
k <- 4
kmeans <- kmeans(tfidf_matrix, k)
kmeans <- kmeans(tfidf_matrix, k)
# Find the most popular words in every cluster
for (i in 1:k) {
s <- sort(kmeans$centers[i,], decreasing=TRUE)
kmeans$cluster[s]
cat(names(s)[1:10], "\n")
}
setwd("~/Documents/IS/lab8")
# Constructs a document-term matrix
dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
mat <- as.matrix(dtm)
# Cluster the documents using the kmeans method with the number of clusters set to 3
k <- 2
kmeansResult <- kmeans(mat, k)
# Find the most popular words in every cluster
for (i in 1:k)
{
s <- sort(kmeansResult$centers[i,], decreasing=T)
cat(names(s)[1:10], "\n")
}
kmeansResult$centers[1,]
mat
dtm
dtm[1,1]
# Remove sparse terms that have at least 70% of empty elements
tdm2 <- removeSparseTerms(tdm, sparse=0.7)
mat <- as.matrix(tdm2)
mat
# The distances between terms can also be calculated using the dist() function
distMatrix <- dist(mat)
# Find clusters of words with hierarchical clustering
fit <- hclust(distMatrix, method="ward.D")
plot(fit)
# Find clusters of words with hierarchical clustering
fit <- hclust(distMatrix, method="ward.D")
setwd("~/Documents/IS/hw02/IntelligentSystems")
dtm <- tm::DocumentTermMatrix(corpus)
dtm <- as.matrix(tm::DocumentTermMatrix(corpus))
dtm
tfidf <- tm::weightTfIdf(dtm)
dtm <- as.matrix(tm::DocumentTermMatrix(corpus), control = list(weighting=weightTfIdf))
dtm
k <- 4
kmeans <- kmeans(dtm, k)
# Find the most popular words in every cluster
for (i in 1:k) {
s <- sort(kmeans$centers[i,], decreasing=TRUE)
cat(names(s)[1:10], "\n")
}
tfidf <- as.matrix(tm::DocumentTermMatrix(corpus), control = list(weighting=weightTfIdf))
k <- 4
kmeans <- kmeans(tfidf, k)
kmeans <- kmeans(tfidf, k)
# Find the most popular words in every cluster
for (i in 1:k) {
s <- sort(kmeans$centers[i,], decreasing=TRUE)
cat(names(s)[1:10], "\n")
}
k <- c(2, 4, 8, 16)
kmeans <- kmeans(tfidf, k)
k <- c(2, 4, 8, 16)
k <- c(2, 4, 8, 16)
for (j in 1:4) {
kmeans <- kmeans(tfidf, k[j])
# Find the most popular words in every cluster
for (i in 1:k[j]) {
s <- sort(kmeans$centers[i,], decreasing=TRUE)
cat(names(s)[1:10], "\n")
}
}
k <- c(2, 4, 8, 16)
kmeans <- vector()
for (j in 1:4) {
kmeans[j] <- kmeans(tfidf, k[j])
}
kmeans[1]$centers[1,]
kmeansResult <- vector()
for (j in 1:4) {
kmeansResult[j] <- kmeans(tfidf, k[j])
}
k <- c(2, 4, 8, 16)
for (j in 1:4) {
kmeansResult <- kmeans(tfidf, k[j])
# Find the most popular words in every cluster
for (i in 1:k[j]) {
s <- sort(kmeansResult$centers[i,], decreasing=TRUE)
cat(names(s)[1:10], "\n")
}
}
kmeansResult <- kmeans(tfidf, 2)
plot(points, main = 'K-Means clustering', col = as.factor(kmeansResult$cluster),
mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0),
xaxt = 'n', yaxt = 'n', xlab = '', ylab = '')
palette <- colorspace::diverge_hcl(2) # Creating a color palette
previous.par <- par(mfrow=c(2,2), mar = rep(1.5, 4))
kmeansResult <- kmeans(tfidf, 2)
plot(points, main = 'K-Means clustering', col = as.factor(kmeansResult$cluster),
mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0),
xaxt = 'n', yaxt = 'n', xlab = '', ylab = '')
par(previous.par) # recovering the original plot space parameters
orspace::diverge_hcl(2) # Creating a color palette
palette <- colorspace::diverge_hcl(2) # Creating a color palette
previous.par <- par(mfrow=c(2,2), mar = rep(1.5, 4))
kmeansResult <- kmeans(tfidf, 2)
kmeansResult <- kmeans(tfidf, 2)
plot(points, main = 'K-Means clustering', col = as.factor(kmeansResult$cluster),
mai = c(0, 0, 0, 0), mar = c(0, 0, 0, 0),
xaxt = 'n', yaxt = 'n', xlab = '', ylab = '')
k <- c(2, 4, 8, 16)
for (j in 1:4) {
kmeansResult <- kmeans(tfidf, k[j])
# Find the most popular words in every cluster
for (i in 1:k[j]) {
s <- sort(kmeansResult$centers[i,], decreasing=TRUE)
cat("for k = ", k, ": ", names(s)[1:10], "\n")
}
cat("\n")
}
kmeansResult <- kmeans(tfidf, k[j])
for (j in 1:4) {
kmeansResult <- kmeans(tfidf, k[j])
# Find the most popular words in every cluster
cat("for k = ", k[j])
for (i in 1:k[j]) {
s <- sort(kmeansResult$centers[i,], decreasing=TRUE)
cat(k[j], ": ", names(s)[1:10], "\n")
}
cat("\n")
}
for (j in 1:4) {
kmeansResult <- kmeans(tfidf, k[j])
# Find the most popular words in every cluster
cat("for k = ", k[j], "\n")
for (i in 1:k[j]) {
s <- sort(kmeansResult$centers[i,], decreasing=TRUE)
cat(names(s)[1:10], "\n")
}
cat("\n")
}
setwd("~/Documents/IS/lab8")
# Constructs a document-term matrix
dtm <- DocumentTermMatrix(corpus, control = list(weighting=weightTfIdf))
mat <- as.matrix(dtm)
# Cluster the documents using the kmeans method with the number of clusters set to 3
k <- 2
kmeansResult <- kmeans(mat, k)
# Find the most popular words in every cluster
for (i in 1:k)
# Find the most popular words in every cluster
for (i in 1:k)
{
s <- sort(kmeansResult$centers[i,], decreasing=T)
cat(names(s)[1:10], "\n")
}
# Cluster the documents using the kmeans method with the number of clusters set to 3
k <- 4
kmeansResult <- kmeans(mat, k)
# Find the most popular words in every cluster
for (i in 1:k)
{
s <- sort(kmeansResult$centers[i,], decreasing=T)
cat(names(s)[1:10], "\n")
}
# Cluster the documents using the kmeans method with the number of clusters set to 3
k <- 8
kmeansResult <- kmeans(mat, k)
# Find the most popular words in every cluster
for (i in 1:k)
{
s <- sort(kmeansResult$centers[i,], decreasing=T)
cat(names(s)[1:10], "\n")
}
setwd("~/Documents/IS/hw02/IntelligentSystems")
# Find the most popular words in every cluster
cat("for k = ", k[j], ":\n")
for (j in 1:4) {
kmeansResult <- kmeans(tfidf, k[j])
# Find the most popular words in every cluster
cat("for k = ", k[j], ":\n")
for (i in 1:k[j]) {
s <- sort(kmeansResult$centers[i,], decreasing=FALSE)
cat(names(s)[1:10], "\n")
}
cat("\n")
}
k <- c(2, 4, 8, 16)
for (j in 1:4) {
kmeansResult <- kmeans(tfidf, k[j])
# Find the most popular words in every cluster
cat("for k = ", k[j], ":\n")
for (i in 1:k[j]) {
s <- sort(kmeansResult$centers[i,], decreasing=FALSE)
cat(names(s)[1:10], "\n")
}
cat("\n")
}
for (j in 1:4) {
kmeansResult <- kmeans(tfidf, k[j])
# Find the most popular words in every cluster
cat("for k = ", k[j], ":\n")
for (i in 1:k[j]) {
s <- sort(kmeansResult$centers[i,], decreasing=TRUE)
cat(names(s)[1:10], length(s), "\n")
}
cat("\n")
}
